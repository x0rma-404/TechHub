{
    "model": "llama3.2:3b",
    "prompt_file": "prompt.txt",
    "temperature": 0.6,
    "stream": false
}